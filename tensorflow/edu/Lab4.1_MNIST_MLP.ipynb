{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tensorflow/mnistdata\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tensorflow/mnistdata\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tensorflow/mnistdata\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tensorflow/mnistdata\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function #Python2.x에서도 print문 실행되도록 하기 위해 추가\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#MNIST 다운로드\n",
    "mnist      = input_data.read_data_sets('/tensorflow/mnistdata', one_hot=True) #숫자 하나만 선택되도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "x_input = tf.placeholder(tf.float32, [None, 784]) # MNIST 이미지 사이즈 28*28=784\n",
    "y_label = tf.placeholder(tf.float32, [None, 10]) # 0-9 범위안의 답 => 10 classes\n",
    "\n",
    "# Weight 정규분포로 초기화 \n",
    "#W1 = tf.Variable(tf.random_normal([784, 100], mean=0.0, stddev=1.0))\n",
    "#b1 = tf.Variable(tf.zeros([100]))\n",
    "#W2 = tf.Variable(tf.random_normal([100, 10], mean=0.0, stddev=1.0))\n",
    "#b2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Weight와 Bias o으로 초기화 실험\n",
    "W1 = tf.Variable(tf.zeros([784, 100]))\n",
    "b1 = tf.Variable(tf.zeros([100]))\n",
    "W2 = tf.Variable(tf.zeros([100, 10]))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Model 정의\n",
    "hidden1 = tf.sigmoid(tf.matmul(x_input, W1) + b1)\n",
    "y_predict = tf.matmul(hidden1, W2) + b2 #분류를 위해 Softmax가 필요하나, 아래 cost함수에서 함께해 줌\n",
    "\n",
    "# Error Cost 계산\n",
    "cross_entropy_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels= y_label, logits= y_predict))\n",
    "\n",
    "# Gradient Descent Optimizer (learning_rate = 0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_cost)\n",
    "\n",
    "# Accuracy 계산\n",
    "correct_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(y_label, 1)) #prediction과 label이 같은지 비교\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 2.306434742 정확도: 0.113499999\n",
      "Epoch: 0002 cost= 2.234250239 정확도: 0.234200001\n",
      "Epoch: 0003 cost= 1.975967582 정확도: 0.295300007\n",
      "Epoch: 0004 cost= 1.844424358 정확도: 0.317699999\n",
      "Epoch: 0005 cost= 1.764755895 정확도: 0.327600002\n",
      "Epoch: 0006 cost= 1.711040838 정확도: 0.328799993\n",
      "Epoch: 0007 cost= 1.675512050 정확도: 0.359299988\n",
      "Epoch: 0008 cost= 1.651705691 정확도: 0.352999985\n",
      "Epoch: 0009 cost= 1.634914764 정확도: 0.355599999\n",
      "Epoch: 0010 cost= 1.622812796 정확도: 0.377700001\n",
      "Epoch: 0011 cost= 1.613449557 정확도: 0.360700011\n",
      "Epoch: 0012 cost= 1.605660343 정확도: 0.354299992\n",
      "Epoch: 0013 cost= 1.599850441 정확도: 0.361600012\n",
      "Epoch: 0014 cost= 1.594505523 정확도: 0.375099987\n",
      "Epoch: 0015 cost= 1.590143454 정확도: 0.380899996\n",
      "Epoch: 0016 cost= 1.586502348 정확도: 0.379099995\n",
      "Epoch: 0017 cost= 1.582784808 정확도: 0.372999996\n",
      "Epoch: 0018 cost= 1.579625169 정확도: 0.377099991\n",
      "Epoch: 0019 cost= 1.577237712 정확도: 0.382600009\n",
      "Epoch: 0020 cost= 1.574640165 정확도: 0.374799997\n",
      "학습 끝!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100 #한번에 가져오는 데이터 수\n",
    "\n",
    "# Graph 생성\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # 변수 초기화\n",
    "\n",
    "    # Training 사이클\n",
    "    for epoch in range(20): #반복횟수\n",
    "        avg_cost = 0. #평균 Cost 변수\n",
    "        \n",
    "        total_batch = int(mnist.train.num_examples / batch_size) #Loop를 도는 횟수 계산\n",
    "        \n",
    "        # Loop 실행\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)  #mnist.train.next_batch 함수로 batch_size만큼 한번에 꺼냄\n",
    "            \n",
    "            # Fit training using batch data\n",
    "            _, cost = sess.run([optimizer, cross_entropy_cost], feed_dict={x_input: batch_xs, y_label: batch_ys})\n",
    "            \n",
    "            # 평균 Cost 계산\n",
    "            avg_cost += cost / total_batch\n",
    "        acc = sess.run(accuracy, feed_dict={x_input: mnist.test.images, y_label: mnist.test.labels})\n",
    "        print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost), \"정확도:\", \"{:.9f}\".format(acc)) #진행상황 출력\n",
    "        \n",
    "    print(\"학습 끝!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
