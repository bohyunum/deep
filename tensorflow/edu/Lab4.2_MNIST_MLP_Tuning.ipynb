{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tensorflow/mnistdata\\train-images-idx3-ubyte.gz\n",
      "Extracting /tensorflow/mnistdata\\train-labels-idx1-ubyte.gz\n",
      "Extracting /tensorflow/mnistdata\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /tensorflow/mnistdata\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function #Python2.x에서도 print문 실행되도록 하기 위해 추가\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#MNIST 다운로드\n",
    "mnist      = input_data.read_data_sets('/tensorflow/mnistdata', one_hot=True) #숫자 하나만 선택되도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "x_input = tf.placeholder(tf.float32, [None, 784]) # MNIST 이미지 사이즈 28*28=784\n",
    "y_label = tf.placeholder(tf.float32, [None, 10]) # 0-9 범위안의 답 => 10 classes\n",
    "\n",
    "# Weight 정규분포로 초기화 \n",
    "W1 = tf.Variable(tf.random_normal([784, 100], mean=0.0, stddev=1.0))\n",
    "b1 = tf.Variable(tf.zeros([100]))\n",
    "W2 = tf.Variable(tf.random_normal([100, 10], mean=0.0, stddev=1.0))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Weight와 Bias o으로 초기화 실험\n",
    "#W1 = tf.Variable(tf.zeros([784, 100]))\n",
    "#b1 = tf.Variable(tf.zeros([100]))\n",
    "#W2 = tf.Variable(tf.zeros([100, 10]))\n",
    "#b2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Model 정의\n",
    "hidden1 = tf.sigmoid(tf.matmul(x_input, W1) + b1)\n",
    "y_predict = tf.matmul(hidden1, W2) + b2 #분류를 위해 Softmax가 필요하나, 아래 cost함수에서 함께해 줌\n",
    "\n",
    "# Error Cost 계산\n",
    "cross_entropy_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels= y_label, logits= y_predict))\n",
    "\n",
    "# Gradient Descent Optimizer (learning_rate = 0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_cost)\n",
    "\n",
    "# Accuracy 계산\n",
    "correct_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(y_label, 1)) #prediction과 label이 같은지 비교\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.927232214 정확도: 0.729300022\n",
      "Epoch: 0002 cost= 0.844115829 정확도: 0.794799984\n",
      "Epoch: 0003 cost= 0.666326751 정확도: 0.826499999\n",
      "Epoch: 0004 cost= 0.576452938 정확도: 0.842899978\n",
      "Epoch: 0005 cost= 0.519582438 정확도: 0.853399992\n",
      "Epoch: 0006 cost= 0.478699039 정확도: 0.862800002\n",
      "Epoch: 0007 cost= 0.448077436 정확도: 0.871800005\n",
      "Epoch: 0008 cost= 0.423173413 정확도: 0.876399994\n",
      "Epoch: 0009 cost= 0.403068990 정확도: 0.881399989\n",
      "Epoch: 0010 cost= 0.386092471 정확도: 0.884999990\n",
      "Epoch: 0011 cost= 0.371416871 정확도: 0.889400005\n",
      "Epoch: 0012 cost= 0.358259916 정확도: 0.890200019\n",
      "Epoch: 0013 cost= 0.347059811 정확도: 0.894500017\n",
      "Epoch: 0014 cost= 0.336442375 정확도: 0.896399975\n",
      "Epoch: 0015 cost= 0.327413758 정확도: 0.898000002\n",
      "Epoch: 0016 cost= 0.319171497 정확도: 0.902199984\n",
      "Epoch: 0017 cost= 0.311347525 정확도: 0.902499974\n",
      "Epoch: 0018 cost= 0.304056399 정확도: 0.904699981\n",
      "Epoch: 0019 cost= 0.297612791 정확도: 0.906799972\n",
      "Epoch: 0020 cost= 0.291338970 정확도: 0.907299995\n",
      "Epoch: 0021 cost= 0.285717502 정확도: 0.908800006\n",
      "Epoch: 0022 cost= 0.279987626 정확도: 0.911899984\n",
      "Epoch: 0023 cost= 0.275067285 정확도: 0.912100017\n",
      "Epoch: 0024 cost= 0.270020030 정확도: 0.912599981\n",
      "Epoch: 0025 cost= 0.265215033 정확도: 0.913500011\n",
      "Epoch: 0026 cost= 0.261147755 정확도: 0.915700018\n",
      "Epoch: 0027 cost= 0.256876161 정확도: 0.915700018\n",
      "Epoch: 0028 cost= 0.252872746 정확도: 0.917400002\n",
      "Epoch: 0029 cost= 0.248883358 정확도: 0.918200016\n",
      "Epoch: 0030 cost= 0.245256727 정확도: 0.918799996\n",
      "Epoch: 0031 cost= 0.241734066 정확도: 0.919000030\n",
      "Epoch: 0032 cost= 0.238360999 정확도: 0.919499993\n",
      "Epoch: 0033 cost= 0.234944462 정확도: 0.921999991\n",
      "Epoch: 0034 cost= 0.231757744 정확도: 0.921500027\n",
      "Epoch: 0035 cost= 0.228738427 정확도: 0.922699988\n",
      "Epoch: 0036 cost= 0.225785698 정확도: 0.923300028\n",
      "Epoch: 0037 cost= 0.222930466 정확도: 0.923500001\n",
      "Epoch: 0038 cost= 0.220075513 정확도: 0.922599971\n",
      "Epoch: 0039 cost= 0.217510822 정확도: 0.926299989\n",
      "Epoch: 0040 cost= 0.214943510 정확도: 0.925999999\n",
      "Epoch: 0041 cost= 0.212467815 정확도: 0.926500022\n",
      "Epoch: 0042 cost= 0.209891462 정확도: 0.927299976\n",
      "Epoch: 0043 cost= 0.207733937 정확도: 0.927699983\n",
      "Epoch: 0044 cost= 0.205324346 정확도: 0.927999973\n",
      "Epoch: 0045 cost= 0.203142349 정확도: 0.927800000\n",
      "Epoch: 0046 cost= 0.200989964 정확도: 0.928699970\n",
      "Epoch: 0047 cost= 0.198935011 정확도: 0.929300010\n",
      "Epoch: 0048 cost= 0.196853163 정확도: 0.929700017\n",
      "Epoch: 0049 cost= 0.194904264 정확도: 0.929000020\n",
      "Epoch: 0050 cost= 0.192841225 정확도: 0.931500018\n",
      "Epoch: 0051 cost= 0.191010381 정확도: 0.931299984\n",
      "Epoch: 0052 cost= 0.189178051 정확도: 0.930800021\n",
      "Epoch: 0053 cost= 0.187154356 정확도: 0.932399988\n",
      "Epoch: 0054 cost= 0.185550455 정확도: 0.931900024\n",
      "Epoch: 0055 cost= 0.183739233 정확도: 0.932799995\n",
      "Epoch: 0056 cost= 0.181996686 정확도: 0.932699978\n",
      "Epoch: 0057 cost= 0.180435461 정확도: 0.932600021\n",
      "Epoch: 0058 cost= 0.178818009 정확도: 0.933700025\n",
      "Epoch: 0059 cost= 0.177321386 정확도: 0.933799982\n",
      "Epoch: 0060 cost= 0.175731039 정확도: 0.934199989\n",
      "학습 끝!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100 #한번에 가져오는 데이터 수\n",
    "# 변수 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Graph 생성\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training 사이클\n",
    "    for epoch in range(60): #반복횟수\n",
    "        avg_cost = 0. #평균 Cost 변수\n",
    "        \n",
    "        total_batch = int(mnist.train.num_examples / batch_size) #Loop를 도는 횟수 계산\n",
    "        \n",
    "        # Loop 실행\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)  #mnist.train.next_batch 함수로 batch_size만큼 한번에 꺼냄\n",
    "            \n",
    "            # Fit training using batch data\n",
    "            _, cost = sess.run([optimizer, cross_entropy_cost], feed_dict={x_input: batch_xs, y_label: batch_ys})\n",
    "            \n",
    "            # 평균 Cost 계산\n",
    "            avg_cost += cost / total_batch\n",
    "        acc = sess.run(accuracy, feed_dict={x_input: mnist.test.images, y_label: mnist.test.labels})\n",
    "        print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost), \"정확도:\", \"{:.9f}\".format(acc)) #진행상황 출력\n",
    "        \n",
    "    print(\"학습 끝!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
